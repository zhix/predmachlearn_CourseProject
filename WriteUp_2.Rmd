---
title: "[Prediction Assignment] Weight Lifting Exercise"
author: "Zhi Xiong Chong"
date: "`r Sys.Date()`"
output: html_document
---

## Introduction 

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=TRUE, warning=FALSE, 
                      message=FALSE, eval=TRUE, cache=TRUE)

```

```{r echo=FALSE}
load('envir.RData')
```


Large amount of data about personal activity is collected using devices in a relatively inexpensive manner. Following the study of Human Activity Recognition (HAR) by Groupware@LES (<http://groupware.les.inf.puc-rio.br/har>), we obtain both the training and test data. 

According to the study, 6 young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Our goal here is to predict the manner in which these 6 individuals did the exercise, dictated by the `classe` variable in the training set. 

This report descibes: 

1. Variables used for prediction. 
2. Method taken to build the prediction model
3. Usage of cross validation
4. Expected out-of-sample error 
5. Reasons for the choices made. 

At the end of this work, the prediction model will be used to predict 20 different test cases. 

Here is the process of Prediction:

1. Question 
2. Input Data 
3. Features 
4. Algorithm 
5. Parameters
6. Evaluation

The above process is followed in this work. Before we begin, we loaded up the following packages and set seed to be `123`. 
```{r echo=TRUE, error=FALSE}
library(ggplot2)
library(caret)
library(randomForest)
library(rattle)
library(rpart.plot)
set.seed(123)

```

## Question
Question: "Can we predict the class of an activity taken?"

Goal: "to predict 20 different test cases labelled under the `problem_id` variable."

## Input Data Loading and Processing
Working directory is set. Testing and training data are downloaded from the url beforehand and loaded as dataframe from the csv file. Testing and training data are briefly looked through to find out about the features available. The outcome to be investigated is labelled as `classe` in the training data. 

```{r eval=FALSE}
wd <- "C:/Users/User/Dropbox/Personal/Coursera/DataScience/8_PredictionMachineLearning/predmachlearn_CourseProject/"
testFile <- paste(wd, "pml-testing.csv", sep="")
trainFile <- paste(wd, "pml-training.csv", sep="")
setwd(wd)

testData <- read.table(testFile, na.strings=c("NA",""),
                      header=TRUE, sep=",")

trainData <- read.table(trainFile, na.strings=c("NA",""),
                       header=TRUE, sep=",")
```

## Data Features

It was realised that there are alot of `NA` and empty values in the datasets. We removed all the columns with `NA` and empty values, and are left with 61 columns. Furthermore, we also removed the first 8 columns that are irrelevant in building the prediction models such as `timestamps`. 

```{r eval=FALSE}
# Build functions that returns amount of NA/empty values in each column
nonNAs <- function(x) {
  as.vector(apply(x, 2, function(x) length(which(!is.na(x)))))
}

# Build vector of columns (with NA/empty values) to drop.
colcounter <- nonNAs(trainData)
drops <- c()
for (count in 1:length(colcounter)) {
  if (colcounter[count] < nrow(trainData)) {
    drops <- c(drops, colnames(trainData)[count])
  }
}

# Drop columns with NA/empty values, then first 8 columns 
trainData <- trainData[,!(names(trainData) %in% drops)]
trainData <- trainData[,8:length(colnames(trainData))]
testData <- testData[,!(names(testData) %in% drops)]
testData <- testData[,8:length(colnames(testData))]
```

In order to ensure all the variables are useful when constructing a prediction model, we look for and remove variables with no variability at all. This can be done through the nearZeroVar function, as shown below. If any `nzv` value is `TRUE`, that particular variable will be removed. As suggested by the results below, no variable shall be removed from the current dataset. 

```{r}
nearZeroVar(trainData, saveMetrics=T)
nearZeroVar(testData, saveMetrics=T)
```

## Algorithm, Parameters and Evaluation: Building and Testing the Prediction Models 

In this study, we will be using 2 different methods in buidling the prediction model, namely the Classification and Regression Trees (CART) and Random Forest (RF). 

### Slicing of data
The training data is sliced into 2 parts: 75% of training data and 25% of testing data within the training data itself. Both the datasets are then sliced further into 5 folds using k-fold partition method.

```{r eval=FALSE}
inTrain <- createDataPartition(y=trainData$classe, p=0.75, list=F)
trainData_train <- trainData[inTrain,]
trainData_test <- trainData[-inTrain,]

folds <- createFolds(y=trainData_train$classe, k=5, list=T, returnTrain=F)
trainData_train1 <- trainData_train[folds[[1]],]
trainData_train2 <- trainData_train[folds[[2]],]
trainData_train3 <- trainData_train[folds[[3]],]
trainData_train4 <- trainData_train[folds[[4]],]
trainData_train5 <- trainData_train[folds[[5]],]

folds <- createFolds(y=trainData_test$classe, k=5, list=T, returnTrain=F)
trainData_test1 <- trainData_test[folds[[1]],]
trainData_test2 <- trainData_test[folds[[2]],]
trainData_test3 <- trainData_test[folds[[3]],]
trainData_test4 <- trainData_test[folds[[4]],]
trainData_test5 <- trainData_test[folds[[5]],]
```

### Training the data to CART model
We constructed the Classification and Regression Trees (CART) model with `rpart` method on 1 of 5 sets of the training data. A summary of the model fitted was summarised and a classification tree plot is shown below.

```{r eval=FALSE}
cartFit1 <- train(classe~., method="rpart", data=trainData_train1)
```
```{r}
print(cartFit1, digit=3)
rattle::fancyRpartPlot(cartFit1$finalModel)
```

### Evaluation of CART model via the Accuracy criterion
We looked closely into the predictivity of the CART model on test values by doing the following: 

```{r}
predictions <- predict(cartFit1, trainData_test1)
confusionMatrix(predictions, trainData_test1$classe)
```

It is realised that the Accuracy value is below 80% -- not favorable in predictivity of a model. When we further tested this on 4 other sliced training datasets, with their respective test values, we tabled each Accuracy as following. All the Accuracies found are roughly within the range of 45-50%. 

```{r eval=FALSE, echo=FALSE}
tableAccuracy <- data.frame(Accuracy=numeric())
tableAccuracy <- rbind(tableAccuracy, confusionMatrix(predictions, trainData_test1$classe)$overall[1])

cartFit <- train(classe~., method="rpart", data=trainData_train2)
testVal = trainData_test2
predictions <- predict(cartFit, testVal)
tableAccuracy <- rbind(tableAccuracy, confusionMatrix(predictions, testVal$classe)$overall[1])

cartFit <- train(classe~., method="rpart", data=trainData_train3)
testVal = trainData_test3
predictions <- predict(cartFit, testVal)
tableAccuracy <- rbind(tableAccuracy, confusionMatrix(predictions, testVal$classe)$overall[1])

cartFit <- train(classe~., method="rpart", data=trainData_train4)
testVal = trainData_test4
predictions <- predict(cartFit, testVal)
tableAccuracy <- rbind(tableAccuracy, confusionMatrix(predictions, testVal$classe)$overall[1])

cartFit <- train(classe~., method="rpart", data=trainData_train5)
testVal = trainData_test5
predictions <- predict(cartFit, testVal)
tableAccuracy <- rbind(tableAccuracy, confusionMatrix(predictions, testVal$classe)$overall[1])

colnames(tableAccuracy) <- "Accuracy"
```

```{r}
tableAccuracy
```

### Training the data to RF model
We then constructed the Random Forest (RF) model with `rf` method on 1 of 5 sets of the training data. A summary of the model fitted was summarised and a classification tree plot is shown below.

```{r eval=FALSE}
rfFit1 <- train(classe~., method="rf", prox=T, data=trainData_train1)
```
```{r}
print(rfFit1, digit=3)
```

### Evaluation of RF model via the Accuracy criterion
Again, we looked closely into the predictivity of the RF model on test values by doing the following: 

```{r}
predictions <- predict(rfFit1, trainData_test1)
confusionMatrix(predictions, trainData_test1$classe)
```

There is a high accuracy (more than 80%) in the model. Similarly, we further tested the RF model on 4 other training datasets, with their respective test values. The accuracy values are tabulated. 

```{r eval=FALSE, echo=FALSE}
tableAccuracy_RF <- data.frame(Accuracy=numeric())
tableAccuracy_RF <- rbind(tableAccuracy_RF, confusionMatrix(predictions, trainData_test1$classe)$overall[1])

rfFit <- train(classe~., method="rf", prox=T, data=trainData_train2)
testVal = trainData_test2
predictions <- predict(rfFit, testVal)
tableAccuracy_RF <- rbind(tableAccuracy_RF, confusionMatrix(predictions, testVal$classe)$overall[1])

rfFit <- train(classe~., method="rf", prox=T, data=trainData_train3)
testVal = trainData_test3
predictions <- predict(rfFit, testVal)
tableAccuracy_RF <- rbind(tableAccuracy_RF, confusionMatrix(predictions, testVal$classe)$overall[1])

rfFit <- train(classe~., method="rf", prox=T, data=trainData_train4)
testVal = trainData_test4
predictions <- predict(rfFit, testVal)
tableAccuracy_RF <- rbind(tableAccuracy_RF, confusionMatrix(predictions, testVal$classe)$overall[1])

rfFit <- train(classe~., method="rf", prox=T, data=trainData_train5)
testVal = trainData_test5
predictions <- predict(rfFit, testVal)
tableAccuracy_RF <- rbind(tableAccuracy_RF, confusionMatrix(predictions, testVal$classe)$overall[1])

colnames(tableAccuracy_RF) <- "Accuracy_RF"
```

```{r}
tableAccuracy_RF
```

### Evaluation of RF model with preprocessing and cross-validation 
We now build the RF models with preprocessing by normalising through `center` and `scale` methods, and with cross-validation through `cv` method. Again the accuracy values are tabulated. The accuracy for each prediction model is recorded in the following table. As you may notice, it is almost the same as the accuracy for purely RF models. 

Due to this, we have decided to go with the refined RF models for our prediction process. 

```{r eval=FALSE}
rfFit1 <- train(classe~., method="rf", prox=T, 
               trControl=trainControl(method = "cv", number = 4),
               preProcess=c("center", "scale"),
               data=trainData_train1)
predictions <- predict(rfFit1, trainData_test1)
confusionMatrix(predictions, trainData_test1$classe)$overall
```


```{r eval=FALSE, echo=FALSE}
tableAccuracy_RFcvnorm <- data.frame(Accuracy=numeric())

testVal <- trainData_test1
trainVal <- trainData_train1
rfFit1 <- train(classe~., method="rf", prox=T, 
               trControl=trainControl(method = "cv", number = 4),
               preProcess=c("center", "scale"),
               data=trainVal)
predictions <- predict(rfFit1, testVal)
tableAccuracy_RFcvnorm <- rbind(tableAccuracy_RFcvnorm, 
                                confusionMatrix(predictions, testVal$classe)$overall[1])

testVal <- trainData_test2
trainVal <- trainData_train2
rfFit2 <- train(classe~., method="rf", prox=T, 
               trControl=trainControl(method = "cv", number = 4),
               preProcess=c("center", "scale"),
               data=trainVal)
predictions <- predict(rfFit2, testVal)
tableAccuracy_RFcvnorm <- rbind(tableAccuracy_RFcvnorm, 
                                confusionMatrix(predictions, testVal$classe)$overall[1])

testVal <- trainData_test3
trainVal <- trainData_train3
rfFit3 <- train(classe~., method="rf", prox=T, 
               trControl=trainControl(method = "cv", number = 4),
               preProcess=c("center", "scale"),
               data=trainVal)
predictions <- predict(rfFit3, testVal)
tableAccuracy_RFcvnorm <- rbind(tableAccuracy_RFcvnorm, 
                                confusionMatrix(predictions, testVal$classe)$overall[1])

testVal <- trainData_test4
trainVal <- trainData_train4
rfFit4 <- train(classe~., method="rf", prox=T, 
               trControl=trainControl(method = "cv", number = 4),
               preProcess=c("center", "scale"),
               data=trainVal)
predictions <- predict(rfFit4, testVal)
tableAccuracy_RFcvnorm <- rbind(tableAccuracy_RFcvnorm, 
                                confusionMatrix(predictions, testVal$classe)$overall[1])

testVal <- trainData_test5
trainVal <- trainData_train5
rfFit5 <- train(classe~., method="rf", prox=T, 
               trControl=trainControl(method = "cv", number = 4),
               preProcess=c("center", "scale"),
               data=trainVal)
predictions <- predict(rfFit5, testVal)
tableAccuracy_RFcvnorm <- rbind(tableAccuracy_RFcvnorm, 
                                confusionMatrix(predictions, testVal$classe)$overall[1])

colnames(tableAccuracy_RFcvnorm) <- "Accuracy_RFcvnorm"
```
```{r}
tableAccuracy_RFcvnorm
```

## Analysing the Accuracy Values
The out-of-sample error is the error generated from applying prediction algorithm to a new dataset. In our case, we define it as `1-Accuracy`, numbers as shown below. 

```{r echo=FALSE, eval=FALSE}
tableAccuracyAll <- cbind(tableAccuracy,tableAccuracy_RF, tableAccuracy_RFcvnorm)

tableAccuracyAll[,4] <- 1 - tableAccuracyAll[,3]
```
```{r echo=FALSE}
colnames(tableAccuracyAll) <- c("Accuracy_CART",
                                "Accuracy_RF",
                                "Accuracy_RFcvnorm", 
                                "Error Rate")
tableAccuracyAll
```

## Conclusion
With 5 RF fitting models applied to predict the 20 items in the testing set, here is what we have found: 

```{r echo=FALSE}
tablePredic <- data.frame(rep("", 20), stringAsFactors=TRUE)

tablePredic[,1] <- predict(rfFit1, testData)
tablePredic[,2] <- predict(rfFit2, testData)
tablePredic[,3] <- predict(rfFit3, testData)
tablePredic[,4] <- predict(rfFit4, testData)
tablePredic[,5] <- predict(rfFit5, testData)
tablePredic[,6] <- testData$problem_id

colnames(tablePredic) <- c("Fit1","Fit2","Fit3","Fit4","Fit5","Problem_id")
tablePredic
```

It is realised that dataset with `problem_id` 3, 8, and 11 have some variations in terms of prediction, while the rest of the datasets have identical prediction results. By comparison, `Fit2` and `Fit5` models give the same results, leaving us with 4 different fitting models. 